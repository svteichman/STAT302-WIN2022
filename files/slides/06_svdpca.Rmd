---
title: "STAT 302 Lecture Slides 6"
subtitle: "Multivariate Data Analysis and PCA"
author: "Sarah Teichman (adapted from slides by Bryan Martin and Peter Gao)"
date: ""
output:
  xaringan::moon_reader:
    css: ["default", "gao-theme.css", "gao-theme-fonts.css"]
    nature:
      highlightStyle: default
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["center"]
---

```{r setup, include=FALSE, purl=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(comment = "##")
knitr::opts_chunk$set(cache = TRUE)
library(kableExtra)
library(tidyverse)
```


# Today's pet picture 


```{r, echo = FALSE, out.width = "700px", fig.align = "center"}
knitr::include_graphics("../../files/images/pets/huating_cat.jpg")
```

Thanks Huating! 

---

# Outline

1. Multivariate data and dimension reduction 
2. Principal Components Analysis (PCA): Theory 
3. Principal Components Analysis (PCA): Application


.middler[**Goal:** Practice using R to implement a statistical method to analyze multivariate data.]

---

class: inverse

.sectionhead[Part 1. Multivariate Data]

---

# Multivariate Data 

* Multi -> Multiple

* Variate -> Variables 

* Multivariate -> Multiple Variables 

--

```{r, echo = FALSE}
data(mtcars)
library(tidyverse)
library(gapminder)
data(gapminder)
```

```{r}
names(mtcars)
names(gapminder)
```

---

# Multivariate Data 

How have we analyzed multivariate data so far in our class? 

```{r}
summary(gapminder$lifeExp)
```

```{r, echo = FALSE, fig.align = 'center', fig.height = 5}
ggplot(gapminder, aes(x = lifeExp)) + 
  geom_histogram(bins = 20, fill = "lightblue", color = "black") + 
  ggtitle("Histogram of Life Expectancy") + 
  xlab("Life Expectancy (in years)") + 
  ylab("Count") + 
  theme_bw(base_size = 14) + 
  theme(plot.title = element_text(hjust = 0.5))
```

---

# Multivariate Data 

How have we analyzed multivariate data so far in our class? 

```{r}
cor(gapminder$lifeExp, gapminder$gdpPercap)
```

```{r, echo = FALSE, fig.align = 'center', fig.height = 5}
ggplot(gapminder, aes(x = lifeExp, y = log(gdpPercap))) + 
  geom_point() + 
  ggtitle("Logged GDP per Capita by Life Expectancy") + 
  xlab("Life Expectancy") + 
  ylab("Logged GDP per capita") + 
  theme_bw(base_size = 14) + 
  theme(plot.title = element_text(hjust = 0.5))
```

---

# Multivariate Data 

How have we analyzed multivariate data so far in our class? 

```{r}
mod <- lm(gdpPercap ~ continent + year + lifeExp + pop,
          data = gapminder)
```

```{r, echo = FALSE, fig.align = 'center', fig.height = 5}
summary(mod)
```

---

# Multivariate Data 

What if we want a way to look at all our variables at once? 

--

What if we want our analysis to consider the relationships between all the variables in our data, not just one or two? 

---

# Multivariate Data 

What if we want a way to look at all our variables at once? 

```{r, message = FALSE, echo = FALSE, fig.align='center', warning=FALSE}
library(GGally)
ggpairs(gapminder, columns = 4:ncol(gapminder))
```

---

# Multivariate Data 

What if we want a way to look at all our variables at once?


--


And we have more than a handful of variables?


--

Consider this dataset with $29$ indicators of environmental performance for $180$ countries.

```{r, echo = FALSE}
environ_df <- read.csv("../projects/02_dimension_reduction/pca_data.csv")
names(environ_df[2:ncol(environ_df)])
```

---


# Multivariate Data 

Dimension reduction: A transformation from a high-dimensional space into a low-dimensional space so that the low-dimensional space retains some meaningful properties of the original data 

--

If we only consider quantitative variables, `gapminder` can be represented in 4 dimensions: 

```{r}
str(gapminder)
```

---

class: inverse

.sectionhead[Part 2. Principal Components Analysis (PCA): Theory]

---

# Principal Components Analysis (PCA)


.middler[**Goal**: Construct a set of variables that collectively retain meaningful properties of the original data set.]

---

# Principal Components Analysis (PCA)


.middler[**Goal**: Construct a set of *uncorrelated linear combinations of the original variables* that collectively retain meaningful properties of the original data set.]

---

# Principal Components Analysis (PCA)


.middler[**Goal**: Construct a set of *uncorrelated linear combinations of the original variables* that collectively retain *most of the variability* of the original data set.]


---

# Principal Components Analysis (PCA)

**Goal**: Construct a set of *uncorrelated linear combinations of the original variables* that collectively retain *most of the variability* of the original data set.

* Principal components: *uncorrelated linear combinations of the original variables*

--

* Principal components are ordered by how much of the variability of the original data they explain

--

* Intuition: If we have $p$ original variables but we can explain most of the variability in our data with $k$ (if $k < p$) principal components, then we can reduce the dimension of our dataset while retaining most of the data's variability 

---

# Principal Components Analysis (PCA)

**Goal**: Construct a set of *uncorrelated linear combinations of the original variables* that collectively retain *most of the variability* of the original data set.

* Define our original variables $\mathbf{x}_1,\ldots, \mathbf{x}_p$, where each $\mathbf{x}_i$ is a vector of length $n$ representing $n$ observations of variable $i$

--

* Define centered versions of our variables $\mathbf{x}^*_1,\ldots, \mathbf{x}^*_p$, where $\mathbf{x}^*_{i} = \mathbf{x}_{i} - \bar{\mathbf{x}}_i$, i.e. each individual observation of variable $i$ has the mean of variable $i$ subtracted off of it.

--

* Our principal components are linear combinations of the $\mathbf{x}^*_i$ vectors:

$$\mathbf{y}_i = l_{i1}\mathbf{x}^*_1 + l_{i2}\mathbf{x}^*_2 + \ldots + l_{ip}\mathbf{x}^*_p$$
---

# Principal Component 1 

* For our first principal component, we want to choose $l_{11},\ldots,l_{1p}$ in order to maximize the variance of $\mathbf{y}_1$.

$$\text{maximize}_{l_{11},\ldots,l_{1p}}\ var(l_{11}\mathbf{x}^*_1 + \ldots + l_{1p}\mathbf{x}^*_p)$$
--

* However, we could always increase the variance of $\mathbf{y}_1$ by just increasing the values of $\mathbf{l}_1$. Let's add the constraint that $\mathbf{l}_1^T\mathbf{l}_1 = 1$ (i.e. $\sum_{j=1}^p l_{1j}^2 = 1$). 

$$\text{maximize}_{l_{11},\ldots,l_{1p}}\ var(l_{11}\mathbf{x}^*_1 + \ldots + l_{1p}\mathbf{x}^*_p)\ \text{s.t.}\  \mathbf{l}_1^T\mathbf{l}_1 = 1$$
--

* We call these scaling values $l_{ij}$ loadings. 
---

# Principal Component 2 


* Let's start with the same condition as the first principal component: 

$$\text{maximize}_{l_{11},\ldots,l_{1p}}\ var(l_{11}\mathbf{x}^*_1 + \ldots + l_{1p}\mathbf{x}^*_p)\ \text{s.t.}\  \mathbf{l}_1^T\mathbf{l}_1 = 1$$
--

* Wait, what about the uncorrelated part? 

$$\text{maximize}_{l_{21},\ldots,l_{2p}}\ var(l_{21}\mathbf{x}^*_1 + \ldots + l_{2p}\mathbf{x}^*_p)\ \text{s.t.}\  \mathbf{l}_2^T\mathbf{l}_2 = 1,\ \mathbf{l}_1^T\mathbf{l}_2 = 0$$
---

# A Brief Geometric Interpretation 

Let's pretend that we only have two original vectors, $\mathbf{x}_1$ and $\mathbf{x}_2$. 

.center[<img src="images/pcagif.gif" alt="" height="375"/>]

.footnote[[Source on stackexchange](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)]


---

# A Brief Geometric Interpretation 

Let's pretend that we only have two original vectors, $\mathbf{x}_1$ and $\mathbf{x}_2$. 

.center[<img src="images/pca2plot.svg" alt="" height="375"/>]


.footnote[[Source on Wikipedia](https://en.wikipedia.org/wiki/Principal_component_analysis)]

---

# The Other Principal Components 

* We can write the same maximization problem for each of the $p$ principal components, adding additional constraints to make sure each component is uncorrelated with all the previous ones. 

--

* For principal component $k$:

$$\text{maximize}_{l_{k1},\ldots,l_{kp}}\ var(l_{k1}\mathbf{x}^*_1 + \ldots + l_{kp}\mathbf{x}^*_p)\ \text{s.t.}\  \mathbf{l}_k^T\mathbf{l}_k = 1, \\ \ \mathbf{l}_1^T\mathbf{l}_k = 0, \mathbf{l}_2^T\mathbf{l}_k = 0, \ldots, \mathbf{l}_{k-1}^T\mathbf{l}_k = 0$$

---

# PCA in Matrix Notation 

* Let's start by returning to PC 1. In order to define $\mathbf{y}_1$, we need to find the loadings that maximize the variance of $\mathbf{y}_1$ subject to the $\mathbf{l}_1^T\mathbf{l}_1 = 1$ constraint. 

--

* Pretend, for now, that $p = 3$. 

\begin{align*}
Var(\mathbf{y}_1) &= Var(l_{11}\mathbf{x}^*_1 + l_{12}\mathbf{x}^*_2 + l_{13}\mathbf{x}^*_3) \\
 &= l_{11}^2Var(\mathbf{x}^*_1) + 2l_{11}l_{12}Cov(\mathbf{x}^*_1,\mathbf{x}^*_2) + 
 2l_{11}l_{13}Cov(\mathbf{x}^*_1,\mathbf{x}^*_3) + \\
 &\ \ \ \ \ \ l_{12}^2Var(\mathbf{x}^*_2) + 2l_{12}l_{13}Cov(\mathbf{x}^*_2,\mathbf{x}^*_3) +\\ &\ \ \ \ \ \  l_{13}^2Var(\mathbf{x}^*_3) 
\end{align*}

--

* Remember the sample covariance matrix for the matrix $\mathbf{x}$.

$$S = \begin{pmatrix}
Var(\mathbf{x}^*_1) & Cov(\mathbf{x}^*_1,\mathbf{x}^*_2) & Cov(\mathbf{x}^*_1,\mathbf{x}^*_3) \\
 Cov(\mathbf{x}^*_1,\mathbf{x}^*_2) & Var(\mathbf{x}^*_2) & Cov(\mathbf{x}^*_2,\mathbf{x}^*_3) \\
 Cov(\mathbf{x}^*_1,\mathbf{x}^*_3) & Cov(\mathbf{x}^*_2,\mathbf{x}^*_3) & Var(\mathbf{x}^*_3) 
\end{pmatrix}$$

---

# PCA in Matrix Notation

* We can rewrite $Var(\mathbf{y}_1)$ in matrix notation using the sample covariance matrix $\mathbf{S}$.

$$Var(\mathbf{y}_1) = \mathbf{l}_1^T\mathbf{S}\mathbf{l}_1$$
$$\mathbf{l}_1^T\mathbf{S}\mathbf{l}_1 = \begin{pmatrix}l_{11} \\ l_{12} \\ l_{13} \end{pmatrix}\begin{pmatrix}
Var(\mathbf{x}^*_1) & Cov(\mathbf{x}^*_1,\mathbf{x}^*_2) & Cov(\mathbf{x}^*_1,\mathbf{x}^*_3) \\
 Cov(\mathbf{x}^*_1,\mathbf{x}^*_2) & Var(\mathbf{x}^*_2) & Cov(\mathbf{x}^*_2,\mathbf{x}^*_3) \\
 Cov(\mathbf{x}^*_1,\mathbf{x}^*_3) & Cov(\mathbf{x}^*_2,\mathbf{x}^*_3) & Var(\mathbf{x}^*_3) 
\end{pmatrix} \begin{pmatrix} l_{11} & l_{12} & l_{13}\end{pmatrix}$$
--

* We can rewrite $Var(\mathbf{y}_k)$ as $Var(\mathbf{y}_k) = \mathbf{l}_k^T\mathbf{S}\mathbf{l}_k$ for any $k \in \{1,\ldots,p\}$.

---

# PCA in Matrix Notation 

* Therefore, we can solve the following maximization problems to get our loadings for our principal components. 

\begin{align*}
\text{maximize}_{\mathbf{l}_1}\ \mathbf{l}_1^T\mathbf{S}\mathbf{l}_1\ &\text{s.t.}\ \mathbf{l}_1^T\mathbf{l}_1=1 \\
\text{maximize}_{\mathbf{l}_2}\ \mathbf{l}_2^T\mathbf{S}\mathbf{l}_2\ &\text{s.t.}\ \mathbf{l}_2^T\mathbf{l}_2=1,\ \mathbf{l}_1^T\mathbf{l}_2=0 \\
&\vdots \\
\text{maximize}_{\mathbf{l}_p}\ \mathbf{l}_p^T\mathbf{S}\mathbf{l}_p\ &\text{s.t.}\ \mathbf{l}_p^T\mathbf{l}_p=1,\ \mathbf{l}_1^T\mathbf{l}_p=0,\\
&\ \ \ \ \ \ \  \mathbf{l}_2^T\mathbf{l}_p=0,\ldots,\mathbf{l}_{p-1}^T\mathbf{l}_{p} = 0
\end{align*}

---

# Finding Loading Vectors for PCA 

* It turns out, the eigendecomposition (also called spectral decomposition) of the sample covariance matrix $\mathbf{S}$ will give us exactly what we need to solve this maximization problems! 

--

* Spectral decomposition theorem: take any square symmetric matrix $\mathbf{A}$. It can be re-written as the decomposition $\mathbf{A} = \mathbf{U\Lambda U}^T$, where $\mathbf{U}$ is an orthonormal matrix made up of the eigenvectors of $\mathbf{A}$ as the columns, and $\mathbf{\Lambda}$ is a diagonal matrix with the eigenvalues of $\mathbf{A}$ on the diagonals.

--

* An orthonormal matrix has the property that for each column $\mathbf{u}_i$, $\mathbf{u}_i^T\mathbf{u}_i = 1$ and for $i \neq j$, $\mathbf{u}_i^T\mathbf{u}_j = 0$.

--

* TLDR (avoiding the linear algebra): The eigendecomposition of our empirical covariance matrix $\mathbf{S} = \mathbf{U\Lambda U}^T$ will give us our variable loadings for our principal components as the columns of the matrix $\mathbf{U}$! 


.footnote[Note: We could also get our loading vectors using the Singular Value Decomposition (SVD) of the matrix X*. These will give the same results!]

---

# Finding Principal Components 

* Remember, that we've defined the first principal component as the vector $\mathbf{y}_i= l_{11}\mathbf{x}^*_1+\ldots + l_{1p}\mathbf{x}^*_p$. 

--

* Therefore, to compute our $\mathbf{y}_i$ vectors, we need to multiply our loadings with our $\mathbf{x}^*_i$ vectors, which are our original variables. 

--

* We can again use matrix multiplication to do this all at once! If $\mathbf{X}^*$ is our $n\times p$ data matrix and $\mathbf{Y}$ is our $n\times p$ matrix of principal component vectors, we can write:
$$\mathbf{Y} = \mathbf{U}\mathbf{X}^*$$

---

# Steps to Perform PCA 

1. Mean center each variable of your data matrix $\mathbf{X}$ to get $\mathbf{X}^*$.

--

2. Compute the empirical covariance matrix of $\mathbf{X}^*$, $\mathbf{S}$.

--

3. Perform an eigendecomposition, $\mathbf{S} = \mathbf{U\Lambda U}^T$.

--

4. Use the columns of $\mathbf{U}$ as the loading vectors, i.e. $\mathbf{l}_i$ for each variable $i\in\{1,\ldots,p\}$.

--

5. Multiple the loadings by the centered matrix $\mathbf{X}^*$ to get the principal component vectors, $\mathbf{Y} = \mathbf{UX}^*$.

---

# Uniqueness of Principal Components 

* One nice property of principal components is that the loadings are unique, up to a sign flip. 
* In other words, there is no randomness (besides signs) when generating principal components. 

---

# Example of PCA 

Let's use the five quantitative variables from the `mtcars` dataset. We'll use the R function `princomp()`. 

```{r}
cars_pca_df <- mtcars %>% 
  select(mpg, disp, drat, wt, qsec)
cars_pca <- princomp(cars_pca_df)
cars_pca$loadings
```

---

# Example of PCA

Let's use the five quantitative variables from the `mtcars` dataset. We'll use the R function `princomp()`. 

```{r}
round(cars_pca$loadings[1:5, ], 3)
apply(cars_pca_df, 2, range)
```

---

# Scaling Variables before PCA 

* Because of this behavior, where principal components depend on the scale of the data, we often scale our variables before performing PCA.
* To scale a variable $\mathbf{x}_i$, first take the centered variable $\mathbf{x}^*_i$, then divide by the standard deviation of variable $i$, to get the centered and scaled version of variable $i$, $\tilde{\mathbf{x}}_i$.

---

# Steps to Perform PCA on Scaled Data 

1. Mean center each variable of your data matrix $\mathbf{X}$ to get $\mathbf{X}^*$.

--

2. Scale each variable of your data matrix $\mathbf{X}^*$ by its standard deviation to get $\tilde{\mathbf{X}}$.

--

3. Compute the empirical covariance matrix of $\tilde{\mathbf{X}}$, $\mathbf{S}$. (Note that this would be equal to the empirical correlation matrix of the original data $\mathbf{X}$!)

--

4. Perform an eigendecomposition, $\mathbf{S} = \mathbf{U\Lambda U}^T$.

--

5. Use the columns of $\mathbf{U}$ as the loading vectors, i.e. $\mathbf{l}_i$ for each variable $i\in\{1,\ldots,p\}$.

--

6. Multiple the loadings by the centered and scaled matrix $\tilde{\mathbf{X}}$ to get the principal component vectors, $\mathbf{Y} = \mathbf{U}\tilde{\mathbf{X}}$.

---

# Back to our example 

```{r}
cars_pca_df <- mtcars %>% 
  select(mpg, disp, drat, wt, qsec)
cars_pca_scaled <- princomp(cars_pca_df, cor = TRUE)
cars_pca_scaled$loadings
```

--- 

# Wait, what about dimension reduction? 

* Remember, the whole point of PCA is to construct a set of *uncorrelated linear combinations of the original variables* that collectively retain *most of the variability* of the original data set.

--

* Our principal components are ordered by the amount of variability they can explain. 

--

* Intuition: If we have $p$ original variables but we can explain most of the variability in our data with $k$ (if $k < p$) principal components, then we can reduce the dimension of our dataset while retaining most of the data's variability 

---

# How do we know how many principal components to use? 

* Define the sum of the variances of variable as $V = \sum_{j=1}^p Var(\mathbf{x}_j)$. 
* The spectral decomposition theorem told us that 
* The sum of the variances of the principal components is equal to $V$.* 

.footnote[*We can prove this using the fact that $\mathbf{X}$ contains the variance of each variable on the diagonals and the expression $\text{trace}(\mathbf{S}) = \text{trace}(\mathbf{U\Lambda U}^T) = \text{trace}(\mathbf{U}^T\mathbf{US}) = \text{trace}(\mathbf{\Lambda}) = \sum_{j=1}^p{\lambda_{j}}.]

