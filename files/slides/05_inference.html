<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>STAT 302, Lecture Slides 5</title>
    <meta charset="utf-8" />
    <meta name="author" content="Sarah Teichman (adapted from slides by Bryan Martin and Peter Gao)" />
    <link href="05_inference_files/remark-css/default.css" rel="stylesheet" />
    <link rel="stylesheet" href="gao-theme.css" type="text/css" />
    <link rel="stylesheet" href="gao-theme-fonts.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, title-slide

# STAT 302, Lecture Slides 5
## Statistical Inference
### Sarah Teichman (adapted from slides by Bryan Martin and Peter Gao)

---




# Today's pet picture 


&lt;img src="../../files/images/pets/huating_cat.jpg" width="700px" style="display: block; margin: auto;" /&gt;

Thanks Huating! 

---
# Outline

1. Hypothesis Testing
2. Linear Models


#### Appendices: 
&lt;ol type="A"&gt;
  &lt;li&gt;Probability&lt;/li&gt;
  &lt;li&gt;Random Variables&lt;/li&gt;
&lt;/ol&gt;


.middler[**Goal:** Understand and learn how to apply statistical tests and models with real data!]

---
class: inverse

.sectionhead[Part 1. Hypothesis Testing]

---
layout: true
# Hypothesis Testing

---

## Motivation

Hypothesis testing always begins with a scientific question of interest.
Suppose we have a coin, and we want to test whether or not it is fair.
We might flip the coin over and over and record the results, and then measure the proportion of times we got a heads.

We can state this experiment more formally. 
Let `\(X_1,...,X_n\)` be `\(n\)` random variables corresponding to indicators of the results of coin flips (i.e. `\(X_i = 1\)` if the `\(i\)`&lt;sup&gt;th&lt;/sup&gt; flip is heads, `\(0\)` if tails).
We don't know the *true* probability that our coin lands on heads, so we may say that `$$X_i \stackrel{iid}{\sim} Bin(1, p),$$` with unknown probability parameter `\(p\)`.&lt;sup&gt;1&lt;/sup&gt;

.footnote[[1] A binomial distribution with `\(n=1\)` is sometimes also denoted `\(Bernoulli(p)\)`]

---

We are interested in whether our coin is fair, which corresponds to `\(p=\dfrac{1}{2}\)`. 
This hypothesis we are interested in testing will be our **null hypothesis**, denoted `\(H_0\)` (pronounced "H-naught" or "H-zero").

Alternatively, we may think that our coin is biased in either direction (i.e. either lands on heads too often or lands on tails too often). 
This corresponds to `\(p \neq \dfrac{1}{2}\)`.
This is our **alternative hypothesis**, denoted `\(H_a\)` (or sometimes `\(H_1\)`).

Thus, our hypothesis test is as follows:

`\begin{align}
H_0: p&amp;=\dfrac{1}{2}\\
H_a: p&amp;\neq \dfrac{1}{2}
\end{align}`

---

## The hypotheses

In general, we can think of

* **Null hypothesis** `\(H_0\)`: our default assumption to be tested, our baseline, no relationships/differences/values of our parameters of interest. *Assumed to be true unless evidence suggests otherwise!*

* **Alternative hypothesis** `\(H_a\)`: the alternative to our null, something interesting is happening in our data, our scientific hypothesis of interest 


An important note: for the most part, these are broad generalizations, not formal definitions! 
It is not always the case that the alternative is "interesting" and the null is not. 
However, we typically set up our hypothesis tests so that this is the case!

---

## The testing

.center[**We always start by assuming that our null hypothesis is true!**]

Our question is whether our data provides sufficient evidence to **reject the null** `\(H_0\)`.

For example, if we flipped our coin 100 times and observed 99 heads, we probably would reject our null hypothesis that the coin is fair.
If instead we flipped our coin 100 times and observed 52 heads, we probably would not conclude that we have enough evidence to reject our null hypothesis that the coin is fair.

.center[**This style of thinking is key in all of statistics!**]

In hypothesis testing, we never, ever, ever examine whether our evidence supports our scientific hypothesis (the alternative)!
We only examine whether it supports rejecting a baseline assumption (the null).

---
layout: false

# Populations and Samples

* The difference between a **population** and a **sample** is fundamental in statistics!

* **Inferential statistics:** using information from a sample to infer something about a population

* "something about a population": we call this a **parameter**. A parameter is a numerical characteristic describing a population/model.


* Example of a population: coin flips distributed `\(Bin(1, p)\)`

* Example of a parameter: the probability of heads `\(p\)`

---

# Populations and Samples

* **Key idea:** the sample is not the population!

* **Inferential statistics:** using information from a sample to infer something about a population

* Use observed data to estimate the parameters

---

# Estimates and Estimators

We use **estimators** to calculate **estimates** for our **parameters**

* **Estimator:** a function of our data (the mean, the sum, the number of heads)

* **Estimate:** the number calculated from our data (an observed sample mean of 52, an observed sum of 117, 61 observed heads)

---
layout: true

# Example

---

Back to our example, recall that we aim to test 

`\begin{align}
H_0: p&amp;=\dfrac{1}{2}\\
H_a: p&amp;\neq \dfrac{1}{2}
\end{align}`

using a random variable representing a heads 
`$$X_i \stackrel{iid}{\sim} Bin(1, p).$$`
Let's assume that we have the patience to flip our coin `\(n=100\)` times.


---


Let's code this in R! (Who has time to flip 100 coins??)


```r
# set the seed for reproducibility!
set.seed(302)
# the true population parameter, considered unknown!
p &lt;- 0.6
# simulate 100 coin flips
coin_flips &lt;- rbinom(100, size = 1, prob = p)
head(coin_flips)
```

```
## [1] 1 0 0 1 1 1
```

```r
sum(coin_flips)
```

```
## [1] 61
```

We observed 61 heads out of 100 coin flips!

This is our **test statistic**, a random variable calculated from our sample data which we will use to evaluate our hypotheses.


---


We observed 61 heads out of 100 coin flips!

Is this unusual enough to reject `\(H_0\)`?

In order to answer this, we must ask ourselves: if we assume `\(H_0\)` is true, how unlikely is what we observed? *Before* we conduct our experiment, we must decide what cut-off we would like to use to determine whether or not to reject `\(H_0\)`. 
This cut-off is typically denoted by the greek letter `\(\alpha\)`.

Typically, we reject `\(H_0\)` if the probability of observing a test statistic *as or more extreme* than what we observed is less than 5%. We will use this cut-off, even though it is arbitrary!
Thus, we set `\(\alpha = 0.05\)`.


---

We can use `pbinom()` to calculate the probability of observing a test statistic as extreme or more extreme than 61. 

* *"as extreme or more extreme"*: Our alternative `\(H_a: p \neq \frac{1}{2}\)` means that we are interested in bias in *either* direction. Thus, we must consider not only the probability of observing *more than* 61 heads, but also the probability of observing *less than* 39 heads, because this is just as extreme in the other direction!

---

Note that when `lower.tail = FALSE`, we return the probability of observing a value *strictly greater than* the value that we input. We want the probability of observing a value *greater than or equal to* the value that we input. 


```r
# record test statistic
test_stat &lt;- sum(coin_flips)
# record how "extreme" our test statistic is
diff_from_exp &lt;- abs(50 - test_stat)
# first component: probability of observing a test statistic as or more extreme
#                  than ours less than the expected value
# second component: probability of observing a test statistic as or more extreme
#                   than ours more than the expected value
p_val &lt;- pbinom(50 - diff_from_exp, size = 100, prob = 0.5) +
  pbinom(50 + diff_from_exp - 1, size = 100, prob = 0.5, lower.tail = FALSE) 
round(p_val, 3)
```

```
## [1] 0.035
```


`\(P(\text{observing a test statistic as or more extreme than what we observed}|H_0) =\)` 0.035!

---

`\(P(\text{observing a test statistic as or more extreme than what we observed}|H_0) =\)` 0.035

This is less than our pre-determined cut-off of 0.05, so we conclude that our results are **statistically significant**! This means that we reject `\(H_0\)` and have evidence to support our alternative hypothesis `\(H_a\)`, that our coin is biased. With that conclusion, we have completed our hypothesis test!

Note that we do **not** conclude that `\(H_0\)` is false, or that `\(H_a\)` is true! We only state our conclusion in terms of sufficient evidence to reject the null or insufficient evidence to reject the null!

---
layout: false
layout: true

# P-values

---

We've already defined a p-value, without explicitly naming the term.

.middler[**p-value:** probability of observing a result as or more extreme than what we observed, assuming the null hypothesis is true]

---

.middler-nocent[
* Tells us whether our results/hypothesis is right or wrong?
]

---

.middler-nocent[
* ~~Tells us whether our results/hypothesis is right or wrong?~~
]

---

.middler-nocent[
* ~~Tells us whether our results/hypothesis is right or wrong?~~

* Probability the null hypothesis is true?
]

---

.middler-nocent[
* ~~Tells us whether our results/hypothesis is right or wrong?~~

* ~~Probability the null hypothesis is true?~~
]

---

.middler-nocent[
* ~~Tells us whether our results/hypothesis is right or wrong?~~

* ~~Probability the null hypothesis is true?~~

* Inverse probability of a publication?
]

---

.middler-nocent[
* ~~Tells us whether our results/hypothesis is right or wrong?~~

* ~~Probability the null hypothesis is true?~~

* ~~Inverse probability of a publication?~~
]

---

.middler[**p-value:** probability of observing a result as or more extreme than what we observed, assuming the null hypothesis is true]

---

.middler[**p-value:** probability of observing a result &lt;u&gt;*as or more extreme*&lt;/u&gt; than what we observed, assuming the null hypothesis is true]


---

**p-value:** probability of observing a result &lt;u&gt;*as or more extreme*&lt;/u&gt; than what we observed, assuming the null hypothesis is true

.center[&lt;img src="images/pval1.png" alt="" height="350"/&gt;]

---

.middler[**p-value:** probability of observing a result as or more extreme than what we observed, assuming the null hypothesis is true]

---

.middler[**p-value:** probability of observing a result as or more extreme than what we observed, &lt;u&gt;*assuming the null hypothesis is true*&lt;/u&gt;]

---

**p-value:** probability of observing a result as or more extreme than what we observed, &lt;u&gt;*assuming the null hypothesis is true*&lt;/u&gt;

.center[&lt;img src="images/pval2.png" alt="" height="350"/&gt;]

---

**p-value:** probability of observing a result as or more extreme than what we observed, assuming the null hypothesis is true

**Premise:** If our results are extreme under the assumption of the null hypothesis, then perhaps our assumption is wrong

---


layout: true

# Type 1 error

---

.middler[## Why we should be cautious about small p-values]

--

**Type 1 errors** are "false positives".

We commit a type 1 error if we reject the null hypothesis when the null hypothesis is actually true.

---

.middler[
`\(\LARGE \alpha = P(\text{reject } H_0 | H_0 \text{ true})\)`

`\(\LARGE 1-\alpha = P(\text{don't reject } H_0 | H_0 \text{ true})\)`
]

---

.middler[
`\(\LARGE .05 = P(\text{reject } H_0 | H_0 \text{ true})\)`

`\(\LARGE .95 = P(\text{don't reject } H_0 | H_0 \text{ true})\)`
]

---

.center[&lt;img src="images/multipletesting.png" alt="" height="500"/&gt;]


---
layout: true

# Type 2 Error
---

.middler[## Why we should be cautious about large p-values]

---

.middler[## Why we should be cautious about large p-values]

**Type 2 errors** are "false negatives".

We commit a type 2 error if we fail to reject the null hypothesis when the null hypothesis is actually false.

---

.middler[
`\(\LARGE P(\text{fail to reject } H_0 | H_0 \text{ false})\)`
]

---

Type 2 error also gives us the concept of **power**, the probability of correctly rejecting the null.

.middler[
`\(\Large \text{Power} = P(\text{reject } H_0 | H_0 \text{ false}) = 1 - P(\text{type 2 error})\)`
]

---
layout: false

# Type 1 and 2 Errors

.middler-nocent[
* We can decrease the probability of a Type 2 errors by increasing our Type 1 error.
  * Why?
* We can decrease the probability of a Type 2 error *without sacrificing* Type 1 error by increasing our sample size.
  * Why?
]


---

# Type 1 and 2 Errors

.middler[
&lt;img src="images/errortable.png" alt="" height="500"/&gt;
]

---
layout: true

# Hypothesis Testing
---


There are many different types of hypothesis tests, but the most common tests have a very simple premise: take your **estimate**, compare it to your **null hypothesized value**, and weight the difference by the **standard error**.

(Note: a standard error is just an estimate of the standard deviation)



---

## What does this look like?

Let's say you are estimating the mean of some data, which you are calling `\(\mu\)`. 
You are testing whether the mean is different than 10. Your **test statistic** is
`$$T=\dfrac{\hat{\mu}-10}{\text{se}(\hat{\mu})},$$`
Note: we put hats on our parameters to indicate our estimates of those parameters. 

For example, `\(\mu\)` is a parameter that represents some unknown population mean. `\(\hat{\mu}\)` is our estimate of `\(\mu\)`, perhaps a sample mean of 11.2.

---

More generally, a common test statistic for an arbitrary parameter `\(\theta\)` is
`$$T = \dfrac{\hat{\theta}-\theta_0}{\text{se}(\hat{\theta})},$$`
where `\(\theta_0\)` is the hypothesized value of `\(\theta\)` under `\(H_0\)`.

We can also test multiple parameters at once (for example `\(H_0: \beta_1 = \beta_2 = 0\)`), but we will not go into the details in this course.

---

## Difference in Means

We can also test for functions of our parameters. 
For example, pretend we are interested in comparing the average height of two different species of trees.
We denote the true average height of tree type 1 and 2 as `\(\mu_1\)` and `\(\mu_2\)`, respectively.
Our null hypothesis, as usual, is that there is no difference. Thus we set up our hypothesis test
`$$\begin{align*}
H_0: \mu_1 &amp;= \mu_2\\
H_a: \mu_1 &amp;\neq \mu_2
\end{align*}$$`

Note that this is equivalent to 
`$$\begin{align*}
H_0: \mu_1 - \mu_2 &amp;= 0\\
H_a: \mu_1 -\mu_2 &amp;\neq 0
\end{align*}$$`

Then one possibility for our test statistic is 
`$$T = \dfrac{\hat{\mu}_1 - \hat{\mu_2}-0}{\text{se}(\hat{\mu}_1 - \hat{\mu_2})}$$`

---

## Bias

**Bias:** The bias of an estimate is the expected difference between the estimate and the true value.
`$$\text{Bias}(\hat{\theta}) = \mathbb{E}(\hat{\theta}) - \theta$$`

Let's say we have `\(T\)` from any of our examples above of the form 
`$$T = \dfrac{\hat{\theta}-\theta_0}{\text{se}(\hat{\theta})}.$$`
Our estimate is called **unbiased** if the expectation of our estimate is equal to the true parameter value, that is, 
`$$\mathbb{E}[\hat{\theta}]=\theta.$$`

---

## We have a test statistic, now what?

If we have a test statistic of the form
`$$T = \dfrac{\hat{\theta}-\theta_0}{\text{se}(\hat{\theta})},$$`
it is very common to model 
`$$T \stackrel{H_0}{\sim} N(0,1).$$`
We won't get into the reasons why in this course&lt;sup&gt;1&lt;/sup&gt;.
Modeling the test statistic with a standard normal distribution is sometimes called a **z-test**.

This means that, *under the null hypothesis*, our test statistic is distributed `\(N(0,1).\)`

.footnote[[1] Commonly, it is due to either the central limit theorem or properties of maximum likelihood estimators.]


---

`$$T \stackrel{H_0}{\sim} N(0,1)$$`

What does the pdf of this distribution look like?

.center[
![](05_inference_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]

---

`$$T \stackrel{H_0}{\sim} N(0,1)$$`

What if we calculate a test statistic `\(T=1.64\)`?

.center[
![](05_inference_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
]

---

If our test is

`$$\begin{align*}
H_0: \theta &amp;= \theta_0\\
H_a: \theta &amp;&gt; \theta_0
\end{align*}$$`

Then our p-value looks like

.center[
![](05_inference_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]

---

If our test is

`$$\begin{align*}
H_0: \theta &amp;= \theta_0\\
H_a: \theta &amp;\neq \theta_0
\end{align*}$$`

Then our p-value looks like

.center[
![](05_inference_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]

---
layout: false

# t-tests

Sometimes, instead of using a normal distribution to model our test statistic, we use a **t distribution**. Typically, this is done when the sample size is small (such as less than 30) or when we estimate our population standard deviation.

A t distribution has heavier tails than a normal distribution, meaning the density is higher in the extreme regions away from the mean.
Thus, using a t distribution is more conservative than using a normal distribution.

*Why?*

---

# t-tests

In the real world, we almost always estimate our population standard deviation. However, both t-tests and z-tests are commonly used.
This is because as our sample size increases, the difference between a t-test and a z-test goes to `\(0\)`.
In practice, we often have sample sizes large enough where there difference is marginal, and normal distributions are nicer to work with than t-distributions.

.center[
![](05_inference_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]

---

# &lt;TT&gt;t.test()&lt;/TT&gt;: t-tests in R

Conducting t-tests in R is very easy! Just use the function `t.test()`. 
For a **two sample t-test** to compare the means of two samples, just provide the two samples in R. 
Here I use randomly simulated data.
This performs a test of the form:

\\[
`\begin{align}
H_0: \mu_x &amp;= \mu_y\\
H_a: \mu_x &amp;\neq  \mu_y 
\end{align}`
\\]


```r
set.seed(302)
x &lt;- rnorm(10, mean = 0, sd = 1)
y &lt;- rnorm(10, mean = 1.5, sd = 1)
t.test(x, y)
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  x and y
## t = -1.5143, df = 16.95, p-value = 0.1484
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -1.627018  0.267543
## sample estimates:
## mean of x mean of y 
## 0.3492983 1.0290359
```

---

# &lt;TT&gt;t.test()&lt;/TT&gt;: t-tests in R

Note that we can also test **one-sided** hypotheses:

\\[
`\begin{align}
H_0: \mu_x &amp;= \mu_y\\
H_a: \mu_x &amp;&lt; \mu_y 
\end{align}`
\\]


```r
t.test(x, y, alternative = "less")
```

```
## 
## 	Welch Two Sample t-test
## 
## data:  x and y
## t = -1.5143, df = 16.95, p-value = 0.07419
## alternative hypothesis: true difference in means is less than 0
## 95 percent confidence interval:
##       -Inf 0.1012809
## sample estimates:
## mean of x mean of y 
## 0.3492983 1.0290359
```

---

# &lt;TT&gt;t.test()&lt;/TT&gt;: t-tests in R

We can also use a **one sample t-test** to compare the means of one sample to some hypothesized value.

This performs a test of the form:

\\[
`\begin{align}
H_0: \mu_x &amp;= 0\\
H_a: \mu_x &amp;\neq 0
\end{align}`
\\]


```r
t.test(x, mu = 0)
```

```
## 
## 	One Sample t-test
## 
## data:  x
## t = 0.98472, df = 9, p-value = 0.3505
## alternative hypothesis: true mean is not equal to 0
## 95 percent confidence interval:
##  -0.4531331  1.1517298
## sample estimates:
## mean of x 
## 0.3492983
```


---

# Today's pet picture 


&lt;img src="../../files/images/pets/kuma.jpg" width="400px" style="display: block; margin: auto;" /&gt;

Thanks Christian! 

---
class: inverse

.sectionhead[Part 2. Linear Models]

---

# Statistical modeling

Let's assume we have some data `\(X_1,\ldots,X_p, Y\)` where `\(X_1,\ldots,X_p\)` are `\(p\)` **independent variables/explanatory variables/covariates/predictors** and `\(Y\)` is the **dependent variable/response/outcome**.
We want to quantify the relationship between our covariates and our response.
Regression provides us with a statistical method to conduct

* **inference:** assess the relationship between our variables, our statistical model as a whole, predictor importance
  * What is the relationship between sleep and GPA?
  * Is parents' education or parents' income more important for explaining income?
* **prediction:** predict new/future outcomes from new/future covariates
  * Can we predict test scores based on hours spent studying?

---
# Intermission: History of "Regression"

.middler[The history of the term "regression", like that of the field of statistics, is closely tied to the eugenics movement.]

---

layout: false

# Linear Regression

Given our response `\(Y\)` and our predictors `\(X_1, \ldots, X_p\)`, a **linear regression model** takes the form:

\\[
\LARGE
`\begin{eqnarray}
&amp;Y &amp;= \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon,\\
&amp;\epsilon &amp;\sim N(0,\sigma^2) 
\end{eqnarray}`
\\]

---

# Matrix Notation

We can fully write out a linear regression model

\\[
`\begin{equation}
\begin{bmatrix} y_1 \\ y_2\\ \vdots \\ y_n \end{bmatrix} = 
\begin{bmatrix} 1 &amp; x_{1,1} &amp; \cdots &amp; x_{1,p}\\
1 &amp; x_{2,1} &amp; \cdots &amp; x_{2, p}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n,1} &amp; \cdots &amp; x_{n, p}\end{bmatrix}
\begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{p} \end{bmatrix} +
\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_{n} \end{bmatrix}
\end{equation}`
\\]

This can also be expressed in matrix form:

\\[
`\begin{align}
\mathbf{Y} &amp;= \mathbf{X}\beta + \epsilon,\\
\epsilon&amp;\sim N(0,1)
\end{align}`
\\]
* `\(\mathbf{Y} \in \mathbb{R}^{n \times 1}\)`: an n-dimensional vector of the response
* `\(\mathbf{X} \in \mathbb{R}^{n \times (p+1)}\)`: a `\(((p+1)\times n)\)` matrix of the predictors (including intercept)
* `\(\beta \in \mathbb{R}^{((p+1)\times 1)}\)`: a `\((p+1)\)`-dimensional vector of regression parameters
* `\(\epsilon \in \mathbb{R}^{n \times 1}\)`: an n-dimensional vector of the error term

This will be helpful for Lab 3!



---

# `\(\large \epsilon\)`: Error term

`\(\epsilon\)`, pronounced epsilon, represents the **error term** of our model.
We can model `\(Y\)` as a linear function of the `\(X\)`'s, but in the real world, the relationship won't always be perfect. There is noise! It can come from

* Measurement error in the `\(X\)`'s
* Measurement error in the `\(Y\)`'s
* Unobserved/missing variables in the model
* Deviations in the true model from linearity
* True randomness

In linear regression, we assume that this error term is normally distributed with mean zero and variance `\(\sigma^2\)`.

---

# `\(\large \beta_0\)`: Intercept

`\(\beta_0\)` is the **intercept term** of our model. Notice that 

`$$\large \mathbb{E}[Y|X_1 = X_2 = \cdots = X_p = 0] = \beta_0$$`

Thus, `\(\beta_0\)` is the expected value of our response if all the covariates are equal to `\(0\)`!
This is also known as the y-intercept of our model.

---

# `\(\large X_j\)`: Independent variable

`\(X_j\)` represents the `\(j\)`&lt;sup&gt;th&lt;/sup&gt; independent variable in our model. Notice that 
`$$\large \mathbb{E}[Y|X_1,\ldots, X_p] = \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p$$`
What happens to this expectation if we increase `\(X_j\)` by 1 unit, holding everything else constant?

--
`$$\large \mathbb{E}[Y|X_1,\ldots, X_j+1,\ldots, X_p] = \beta_0 + \beta_1 X_1 + \cdots + (X_j+1)\beta_j + \cdots + \beta_p X_p$$`
--

`$$\large \mathbb{E}[Y|X_1,\ldots, X_j+1,\ldots, X_p]  -  \mathbb{E}[Y|X_1,\ldots, X_j,\ldots, X_p] = \beta_j$$`



The conditional expectation of `\(Y\)` increases by `\(\beta_j\)`!

---

# `\(\large \beta_j\)`: Coefficient

`\(\beta_j\)` represents the `\(j\)`&lt;sup&gt;th&lt;/sup&gt; regression coefficient in our model.
From the previous slide, we saw that for every 1 unit increase in `\(X_j\)`, holding all other variables constant, the expected value of the response increases by `\(\beta_j\)`.
From this we can derive an interpretation.

**Interpretation of `\(\beta_j\)`:** the expected difference in the response between two observations differing by one unit in `\(X_j\)`, with all other covariates identical.

---

# Linear Regression


\\[
\LARGE
`\begin{eqnarray}
&amp;Y &amp;= \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon,\\
&amp;\epsilon &amp;\sim N(0,\sigma^2) 
\end{eqnarray}`
\\]


* `\(Y\)`: dependent variable, outcome, response
* `\(X_j\)`: independent variable, covariate, predictor
* `\(\beta_0\)`: Intercept
* `\(\beta_j\)`: coefficient, the expected difference in the response between two observations differing by one unit in `\(X_j\)`, with all other covariates identical.
* `\(\epsilon\)`: error, noise, with mean `\(0\)` and variance `\(\sigma^2\)`

---

# &lt;TT&gt;lm():&lt;/TT&gt; Linear Model

We fit a linear regression model in R using `lm()`. 
The first argument is a **formula**, which is a type of R object. 
Formulas typically take the following form: `Y ~ X_1 + X_2 + ... + X_p`.

The dependent variable, `Y` goes on the left-hand side of the tilde `~`, which marks the formula. 
The independent variables are added on the right-hand side.
Using this formula will give us a model in the form of
\\[
`\begin{eqnarray}
&amp;Y &amp;= \beta_0 + \beta_1 X_1 + \cdots + \beta_p X_p + \epsilon,\\
&amp;\epsilon &amp;\sim N(0,\sigma^2) 
\end{eqnarray}`
\\]

```r
lm(Y ~ X_1 + X_2 + ... + X_p)
```


---

# &lt;TT&gt;lm():&lt;/TT&gt; Linear Model


```r
data(mtcars)
head(mtcars, 3)
```

```
##                mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4     21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710    22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
```

```r
my_lm &lt;- lm(mpg ~ hp + wt, data = mtcars)
class(my_lm)
```

```
## [1] "lm"
```

```r
my_lm
```

```
## 
## Call:
## lm(formula = mpg ~ hp + wt, data = mtcars)
## 
## Coefficients:
## (Intercept)           hp           wt  
##    37.22727     -0.03177     -3.87783
```

---

# &lt;TT&gt;lm():&lt;/TT&gt; Linear Model
We can see from `names()` that `lm` objects contain a lot more than they print out by default.

```r
names(my_lm)
```

```
##  [1] "coefficients"  "residuals"     "effects"       "rank"         
##  [5] "fitted.values" "assign"        "qr"            "df.residual"  
##  [9] "xlevels"       "call"          "terms"         "model"
```


---

# &lt;TT&gt;summary():&lt;/TT&gt; Model summaries

`summary()` gives us a summary of our `lm` object in R.

* The quantiles of the residuals: hopefully, they match a normal distribution!
* Coefficients, their standard errors, and their individual significances
* (Adjusted) R-squared value: how much of the overall variability in the response is explained by the model?
* F-statistic: hypothesis test for the significance of the overall model

---

# &lt;TT&gt;summary():&lt;/TT&gt; Model summaries


```r
summary(my_lm)
```

```
## 
## Call:
## lm(formula = mpg ~ hp + wt, data = mtcars)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -3.941 -1.600 -0.182  1.050  5.854 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 37.22727    1.59879  23.285  &lt; 2e-16 ***
## hp          -0.03177    0.00903  -3.519  0.00145 ** 
## wt          -3.87783    0.63273  -6.129 1.12e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.593 on 29 degrees of freedom
## Multiple R-squared:  0.8268,	Adjusted R-squared:  0.8148 
## F-statistic: 69.21 on 2 and 29 DF,  p-value: 9.109e-12
```


---

# &lt;TT&gt;plot():&lt;/TT&gt; Regression model diagnostics


Calling `plot(my_lm)` will return several diagnostic plots.
Remember that we want our error term to look normally distributed with mean zero.
Interpreting these is both a science and an art.
We won't go into all the details for this class, but here are some tips:

---

# &lt;TT&gt;plot():&lt;/TT&gt; Regression model diagnostics

**Residuals vs Fitted:** these are your errors (residuals) plotted over the predicted outcome (fitted). Errors should be random, so here you want to see randomly scattered points with no discernable pattern. You want the trend line to be approximately horizontal.
&lt;img src="05_inference_files/figure-html/unnamed-chunk-17-1.png" style="display: block; margin: auto;" /&gt;

---

# &lt;TT&gt;plot():&lt;/TT&gt; Regression model diagnostics

**Normal Q-Q plot:** These are the quantiles of your errors against the quantiles of a normal distribution. In theory, your errors should be normally distributed, so you are hoping that points are mostly along the 45-degree `\(y=x\)` line.
&lt;img src="05_inference_files/figure-html/unnamed-chunk-18-1.png" style="display: block; margin: auto;" /&gt;

---

# &lt;TT&gt;plot():&lt;/TT&gt; Regression model diagnostics

**Scale-location:** This looks at the magnitude of standardized residuals over the predicted outcome. Similar interpretation as residuals vs fitted. This can make it slightly easier to identify undesireable patterns.
&lt;img src="05_inference_files/figure-html/unnamed-chunk-19-1.png" style="display: block; margin: auto;" /&gt;

---

# &lt;TT&gt;plot():&lt;/TT&gt; Regression model diagnostics

**Residuals vs leverage:** This can help identify highly influential points, such as outliers. If points are outside dotted red lines, then removing them would noticeably alter your results. *Never just remove outliers!* If it's clearly mis-entered data, fine. It is more important to understand why outliers are there than to remove them.
&lt;img src="05_inference_files/figure-html/unnamed-chunk-20-1.png" style="display: block; margin: auto;" /&gt;

--- 

# &lt;TT&gt;coef():&lt;/TT&gt; Extract coefficients

Use `coef()` to extract estimated coefficients as a vector.


```r
coef(my_lm)
```

```
## (Intercept)          hp          wt 
## 37.22727012 -0.03177295 -3.87783074
```

---

# &lt;TT&gt;coef():&lt;/TT&gt; Extract coefficients

Use `coef()` to extract estimated coefficients as a vector.


```r
coef(my_lm)
```

```
## (Intercept)          hp          wt 
## 37.22727012 -0.03177295 -3.87783074
```

---

# &lt;TT&gt;fitted():&lt;/TT&gt; Extract fitted values

Use `fitted()` to extract the fitted/estimated values for the response. 
It can be useful to compare our fitted values to actual values to help assess model fit.


```r
mod_fits &lt;- fitted(my_lm)
my_df &lt;- data.frame(actual = mtcars$mpg, fitted = mod_fits)
ggplot(my_df, aes(x = fitted, y = actual)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0, col = "red", lty = 2) + 
  theme_bw(base_size = 15) +
  labs(x = "Fitted values", y = "Actual values", title = "Actual vs. Fitted") +
  theme(plot.title = element_text(hjust = 0.5)) + 
  xlim(c(0, 30) + ylim(c(0,35)) 
```


.center[
![](05_inference_files/figure-html/unnamed-chunk-24-1.png)&lt;!-- --&gt;
]

---

# &lt;TT&gt;predict():&lt;/TT&gt; Predict new outcomes

Use `predict()` to predict new outcomes given new explanatory variables.
For example, pretend we observe two new cars with horsepowers of `100` and `150`, respectively, and weights of `3000` and `3500`, respectively.


```r
# Note: wt is in 1000s of lbs!
new_obs &lt;- data.frame(hp = c(100, 150), wt = c(3, 3.5))
predict(my_lm, new_obs)
```

```
##        1        2 
## 22.41648 18.88892
```

We'll come back to prediction in future lectures!

---
layout: true

# Formula Tools
---

## `- 1`

Use `- 1` to remove an intercept from your model. Only do this if you are very sure that what you are doing is appropriate! I don't recommend doing this in practice.


```r
no_intercept &lt;- lm(mpg ~ hp + wt - 1, data = mtcars)
summary(no_intercept)
```

```
## 
## Call:
## lm(formula = mpg ~ hp + wt - 1, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -19.407  -2.382   2.511   7.091  23.885 
## 
## Coefficients:
##    Estimate Std. Error t value Pr(&gt;|t|)   
## hp -0.03394    0.03940  -0.861   0.3959   
## wt  6.84045    1.89425   3.611   0.0011 **
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 11.32 on 30 degrees of freedom
## Multiple R-squared:  0.7264,	Adjusted R-squared:  0.7082 
## F-statistic: 39.83 on 2 and 30 DF,  p-value: 3.599e-09
```

---

## `*`

Use `X1*X2` to include an interaction effect in your model. This is useful if you have reason to believe two covariates interact, such as gender and education in a wage model.
In our case, we'll assume weight and transmission (0 for automatic, 1 for manual) interact in their effect on mpg.

When you use `X1*X2` you include an interaction effect and marginal effects for each variable `X1` and `X2`. If you ever want to exclude these marginal effects (this will happen rarely!) you can instead use `X1:X2`.

---


```r
interact &lt;- lm(mpg ~ wt*am, data = mtcars)
summary(interact)
```

```
## 
## Call:
## lm(formula = mpg ~ wt * am, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.6004 -1.5446 -0.5325  0.9012  6.0909 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  31.4161     3.0201  10.402 4.00e-11 ***
## wt           -3.7859     0.7856  -4.819 4.55e-05 ***
## am           14.8784     4.2640   3.489  0.00162 ** 
## wt:am        -5.2984     1.4447  -3.667  0.00102 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 2.591 on 28 degrees of freedom
## Multiple R-squared:  0.833,	Adjusted R-squared:  0.8151 
## F-statistic: 46.57 on 3 and 28 DF,  p-value: 5.209e-11
```

---

## `.`

Use `~ .` to include all non-response variables in the input data as independent variables.


```r
include_all &lt;- lm(mpg ~ ., data = mtcars)
summary(include_all)$coefficients
```

```
##                Estimate  Std. Error    t value   Pr(&gt;|t|)
## (Intercept) 12.30337416 18.71788443  0.6573058 0.51812440
## cyl         -0.11144048  1.04502336 -0.1066392 0.91608738
## disp         0.01333524  0.01785750  0.7467585 0.46348865
## hp          -0.02148212  0.02176858 -0.9868407 0.33495531
## drat         0.78711097  1.63537307  0.4813036 0.63527790
## wt          -3.71530393  1.89441430 -1.9611887 0.06325215
## qsec         0.82104075  0.73084480  1.1234133 0.27394127
## vs           0.31776281  2.10450861  0.1509915 0.88142347
## am           2.52022689  2.05665055  1.2254035 0.23398971
## gear         0.65541302  1.49325996  0.4389142 0.66520643
## carb        -0.19941925  0.82875250 -0.2406258 0.81217871
```

---
layout: false

# Reminder of Matrix Notation

We can fully write out a linear regression model

\\[
`\begin{equation}
\begin{bmatrix} y_1 \\ y_2\\ \vdots \\ y_n \end{bmatrix} = 
\begin{bmatrix} 1 &amp; x_{1,1} &amp; \cdots &amp; x_{1,p}\\
1 &amp; x_{2,1} &amp; \cdots &amp; x_{2, p}\\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
1 &amp; x_{n,1} &amp; \cdots &amp; x_{n, k}\end{bmatrix}
\begin{bmatrix} \beta_0 \\ \beta_1 \\ \vdots \\ \beta_{p} \end{bmatrix} +
\begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_{n} \end{bmatrix}
\end{equation}`
\\]

This can also be expressed in matrix form:

\\[
`\begin{align}
\mathbf{Y} &amp;= \mathbf{X}\beta + \epsilon,\\
\epsilon&amp;\sim N(0,1)
\end{align}`
\\]
* `\(\mathbf{Y} \in \mathbb{R}^{n \times 1}\)`: an n-dimensional vector of the response
* `\(\mathbf{X} \in \mathbb{R}^{n \times (p+1)}\)`: a `\(((p+1)\times n)\)` matrix of the predictors (including intercept)
* `\(\beta \in \mathbb{R}^{((p+1)\times 1)}\)`: a `\((p+1)\)`-dimensional vector of regression parameters
* `\(\epsilon \in \mathbb{R}^{n \times 1}\)`: an n-dimensional vector of the error term


---

# Least squares

How do we estimate `\(\beta_{(p+1)\times1}\)`?

Least squares finds values of `\(\widehat{\beta}_0, \widehat{\beta}_1,\dots,\widehat{\beta_p}\)` that define a (multidimensional) line that minimizes the squared distance between the points and the line, i.e. makes `$$\left[y_i - (\widehat{\beta}_0 + \widehat{\beta}_1 x_{i, 1} + \dots + \widehat{\beta}_p x_{i,p})\right]^2$$` as small as possible for all `\(i = 1, \dots, n\)`.

The vector `\(\widehat{\beta}\)` that minimizes the sum of the squared distances is:

$$ \widehat{\beta}=\left(\mathbf{X}^T \mathbf{X} \right)^{-1}\mathbf{X}^T \mathbf{Y}.$$

Note: In statistics, once we have estimated a parameter we put a "hat" on it, e.g. `\(\widehat{\beta_0}\)` is the estimate of the true parameter `\(\beta_0\)`.

---

# Least squares in R

For lab 3, you will need to take a `formula` and an input data frame `data` and perform linear regression.

Your function should return a `table` similar to the coefficent table from `summary()` with rows for each coefficient (including the `(Intercept)`!) and columns for the `Estimate`, `Std. Error`, `t value`, and `Pr(&gt;|t|)`. There should be row and column names.


```r
my_lm &lt;- function(formula, data) {
  # YOUR CODE HERE
}
```

Tips:
1. Start by writing pseudo-code. What are the steps you need to achieve?
2. Remember the formula `\(\widehat{\beta}=\left(\mathbf{X}^T \mathbf{X} \right)^{-1}\mathbf{X}^T \mathbf{Y}\)`. How will you extract `\(\mathbf{Y}\)` and `\(\mathbf{X}\)` from your data frame? The functions `model.frame()` and `model.response()` will be helpful.


---
class: inverse

.sectionhead[Appendix A. Probability]

---

.middler[A *very* brief overview...]

---

# Sample Space

A **sample space**, commonly denoted `\(\Omega\)` or `\(S\)`, is the set of all possible outcomes from a random experiment. For example

* Coin flip: `\(\Omega = \{H, T\}\)`
* Two coin flips: `\(\Omega = \{HH, HT, TH, TT\}\)`
* Rolling a 6-sided die: `\(\Omega = \{1, 2, 3, 4, 5, 6\}\)`
* Hours spent sleeping in day: `\(\Omega = \{x: x\in \mathbb{R}, 0 \leq x \leq 24\}\)`
* A simulation from a normal distribution: `\(\Omega = (-\infty, \infty)\)`

The elements of `\(\Omega\)` must be **mutually exclusive** (no overlap) and **collectively exhaustive** (cover all potential options).

---

# Events

An **event**, which we will call `\(A\)`, can be any subset of your sample space. For example

* Heads in a coin flip: `\(A = \{H\}\)`
* At least one heads in two coin flips: `\(A = \{HT, HT, TH\}\)`
* Rolling an even number on a 6-sided die: `\(A = \{2, 4, 6\}\)`
* Sleeping at least 8 hours in a day: `\(A = \{x: x \in \mathbb{R}, 8 \leq x \leq 24\}\)`
* Simulating a number between 1 and 2, inclusive, from a normal distribution: `\(A = [1, 2]\)`

---

# Probability

Informally, probability `\(P\)` is often defined as the chance of something happening.

More formally, it is a function that goes from an event `\(A\)` to the real line. (You might see this written `\(P: A \mapsto \mathbb{R}\)`)

* `\(P(\text{heads in a fair coin flip}) = \dfrac{1}{2}\)`
* `\(P(\text{at least one heads in two fair coin flips}) = \dfrac{3}{4}\)` 
* `\(P(\text{rolling an even number on a fair dice}) = \dfrac{1}{2}\)`

---

# Axioms of Probability

Probability allows follows three basic principles, known as the **axioms of probability**.

1. The probability of any event `\(A\)` must be between 0 and 1, inclusive.
  * `\(0 \leq P(A)\leq 1\)`
2. The probability of the sample space is equal to 1.
  * `\(P(\Omega) = 1\)`
3. If events `\(A\)` and `\(B\)` are **mutually exclusive**/**disjoint**, then the probability of *either* `\(A\)` *or* `\(B\)` is the same as the sum of the probability of `\(A\)` and the probability of `\(B\)`.
  * `\(A \cap B = \emptyset \ \ \Rightarrow \ \ P(A\cup B) = P(A) + P(B)\)`

---
layout: true

# Probability Notation

---

## Intersection: `\(\cap\)`

`\(P(A \cap B)\)`: *joint* probability of `\(A\)` *and* `\(B\)`.

.center[&lt;img src="images/set.png" alt="" height="350"/&gt;]

---

## Intersection: `\(\cap\)`

`\(P(A \cap B)\)`: *joint* probability of `\(A\)` *and* `\(B\)`.

.center[&lt;img src="images/intersect.png" alt="" height="350"/&gt;]

---

## Union: `\(\cup\)`

`\(P(A \cup B)\)` probability of `\(A\)` *or* `\(B\)`.

.center[&lt;img src="images/set.png" alt="" height="350"/&gt;]

---

## Union: `\(\cup\)`

`\(P(A \cup B)\)` probability of `\(A\)` *or* `\(B\)`.

.center[&lt;img src="images/union.png" alt="" height="350"/&gt;]

---


## Complement: `\(A^c, A'\)`

`\(P(A^c)\)` probability of *not* `\(A\)`

.center[&lt;img src="images/set.png" alt="" height="350"/&gt;]

---

## Complement: `\(A^c, A'\)`

`\(P(A^c)\)` probability of *not* `\(A\)`

.center[&lt;img src="images/complement.png" alt="" height="350"/&gt;]

---


## Difference: `\(A\setminus B\)`

`\(P(A\setminus B)\)` probability of `\(A\)` *and not* `\(B\)`

.center[&lt;img src="images/set.png" alt="" height="350"/&gt;]

---

## Difference: `\(A\setminus B\)`

`\(P(A\setminus B)\)` probability of `\(A\)` *and not* `\(B\)`

.center[&lt;img src="images/difference.png" alt="" height="350"/&gt;]

---

## Conditional: `\(A | B\)`

`\(P(A | B)\)` probability of `\(A\)` *conditional on*/*given* `\(B\)`

.center[&lt;img src="images/set.png" alt="" height="350"/&gt;]

---

## Conditional: `\(A | B\)`

`\(P(A | B)\)` probability of `\(A\)` *conditional on*/*given* `\(B\)`

.center[&lt;img src="images/conditional.png" alt="" height="350"/&gt;]

---

## Subset: `\(A \subseteq \Omega\)`

`\(A \subseteq \Omega\)`: `\(A\)` is a *subset* of `\(\Omega\)`

`\(A \subset \Omega\)`: `\(A\)` is a *proper subset* of `\(\Omega\)`

.center[&lt;img src="images/subset.png" alt="" height="350"/&gt;]

---

## Subset: `\(A \subseteq \Omega\)`

`\(A \subseteq \Omega\)`: `\(A\)` is a *subset* of `\(\Omega\)`

`\(A \subset \Omega\)`: `\(A\)` is a *proper subset* of `\(\Omega\)`

.center[&lt;img src="images/nsubset.png" alt="" height="350"/&gt;]

---

## Superset: `\(\Omega \supseteq A\)`

`\(\Omega \supseteq A\)`: `\(A\)` is a *superset* of `\(\Omega\)`

`\(\Omega \supset A\)`: `\(A\)` is a *proper superset* of `\(\Omega\)`

.center[&lt;img src="images/supset.png" alt="" height="350"/&gt;]

---

## Superset: `\(\Omega \supseteq A\)`

`\(\Omega \supseteq A\)`: `\(A\)` is a *superset* of `\(\Omega\)`

`\(\Omega \supset A\)`: `\(A\)` is a *proper superset* of `\(\Omega\)`

.center[&lt;img src="images/nsupset.png" alt="" height="350"/&gt;]

---

## Element of: `\(X \in A\)`

`\(X \in A\)`: `\(X\)` is an *element of* `\(A\)`

.center[&lt;img src="images/element.png" alt="" height="350"/&gt;]

---

## Empty set: `\(\varnothing\)`

`\(A\cap E = \varnothing\)`: the intersection of `\(A\)` and `\(E\)` is the empty set

.center[&lt;img src="images/empty.png" alt="" height="350"/&gt;]

---
layout: false

# Identities of Probability

* The probability of `\(A^c\)` is `\(1\)` minus the probability of `\(A\)`
  * `\(P(A^c) = 1-P(A)\)`
  
* If `\(A\)` is a subset of `\(B\)`, then the probability of `\(A\)` is less than or equal to the probability of `\(B\)`
  * `\(A \subseteq B \implies P(A)\leq P(B)\)`
  
* The probability of a union is equal to the sum of the probabilities minus the probability of an intersection
  * `\(P(A \cup B) = P(A) + P(B) - P(A\cap B)\)`

---

# De Morgan's Laws

* Complement of the union is equal to the intersection of the complements
  
.center[&lt;img src="images/demorgan1.png" alt="" height="350"/&gt;]

---

# De Morgan's Laws

* Complement of the intersection is equal to the union of the complements
  
.center[&lt;img src="images/demorgan2.png" alt="" height="350"/&gt;]

---

# De Morgan's Laws

1. Complement of the union is equal to the intersection of the complements
  * `\((A \cup B)^c = A^c \cap B^c\)`
  
2. Complement of the intersection is equal to the union of the complements
  * `\((A \cap B)^c = A^c \cup B^c\)`
  
.center[&lt;img src="images/demorgan1.png" alt="" height="250"/&gt;    &lt;img src="images/demorgan2.png" alt="" height="250"/&gt;]

---

# Independence

We say that two events `\(A\)` and `\(B\)` are independent, `\(A\perp \!\!\! \perp B\)`, *if and only if*

* `\(P(A \cap B) = P(A) P(B)\)`

* `\(P(A|B) = P(A)\)`

* `\(P(B|A) = P(B)\)`

This is an *extremely* important concept in statistics!

---

# Conditional Probability 

The conditional probability of `\(A\)` given `\(B\)` is equal to the joint probability of `\(A\)` and `\(B\)` divided by the marginal probability of `\(B\)`

`$$P(A|B) = \dfrac{P(A\cap B)}{P(B)}$$`

Note that this implies 

`$$P(A\cap B) = P(A|B) P(B)$$`


---


# Bayes' Rule

`$$P(A|B) = \dfrac{P(A\cap B)}{P(B)}$$`
also implies
`$$P(A|B) = \dfrac{P(B|A)P(A)}{P(B)}$$`
commonly known as **Bayes' rule**!

We won't get into the details in this class, but this can be a very useful result for reversing the conditions in your analysis.

For example: There is a big difference between the probability of having a disease given a positive screening, and the probability of a positive screening given a disease! These concepts are often confused in popular media!

---

# Law of Total Probability

We say that a set of events is a **partition** if:

* The set does not contain the empty set
* The union of the events in the set is equal to the sample space
* The intersection of any two distinct events in the set is equal to the empty set

Note that an event and its complement always define a partition!

.center[&lt;img src="images/partition1.png" alt="" height="350"/&gt;]

---

# Law of Total Probability

The **law of total probability** states that given a partition `\(P_1, P_2, \ldots, P_n\)`, then 
`$$P(A) = P(A|P_1)P(P_1) + P(A|P_2)P(P_2) + \cdots +  P(A|P_n)P(P_n)$$`

Commonly, our partition is some event `\(B\)` and its complement!
`$$P(A) = P(A|B)P(B) + P(A|B^c)P(B^c)$$`

.center[&lt;img src="images/partition2.png" alt="" height="350"/&gt;]

---
class: inverse

.sectionhead[Appendix B. Random Variables]

---

# Random Variables

Typically, we don't care about specific events occuring.
Instead, we tend to focus on functions of our events.
These functions are called **random variables**.

More formally&lt;sup&gt;1&lt;/sup&gt;, a random variable `\(X\)` can be defined as function `\(X: \Omega \mapsto \mathbb{R}\)`.

* the number of heads out of 10 coin flips
* the sum of 8 standard die rolls
* the average value of 1,000 simulations from a `\(\mathcal{N}(0,1)\)`

Typically, a random variable is denoted by a uppercase letter, such as `\(X\)`, and values that the random variable takes is denoted by a lowercase letter, such as `\(x\)`. 
For example, we might ask the `\(P(X=x)\)` for multiple values of `\(x\)`.

We call the set of all values a random variable can take the **support** of that random variable.

.footnote[[1] still not all the way formal]

---

# Random Variables

Random variables are **not** events! 

This can be confusing because often use similar notation with random variables and events. Think of an event as an outcome that can lead to a certain value of a random variable. For example:

* Event in 10 coinflips: `\(\{THHTTTHTTH\}\)`. 
  * Random variable representing the number of heads `\(X = 4\)`.
  
* Event in 8 standard die rolls: `\(\{2, 4, 2, 1, 5, 4, 2, 6\}\)`. 
  * Random variable representing the sum `\(X = 25\)`.

---

# Discrete Random Variables 

Random variables are **discrete** if there are a finite&lt;sup&gt;1&lt;/sup&gt; number of values in the support. 
We've already seen some examples of this in class from the binomial distribution!
We define the **probability mass function**, or **PMF**, of a discrete random variable `\(X \sim Bin(n,p)\)` as 
`$$P(X=k|n,p) = \begin{pmatrix} n\\ k\end{pmatrix} p^k(1-p)^{n-k}$$`

Probability mass functions must satisfy:

1. `\(0 \leq P(X=x) \leq 1\)` for all `\(x\)`
2. `\(\sum_{x \in support(X)} P(X = x) = 1\)`
3. For any set `\(A\subseteq support(X)\)`, `\(P(X \in A) = \sum_{x \in A} P(X = x)\)`

.footnote[[1] or countably infinite]

---

# Continuous Random Variables

Random variables are **continuous** if the support is uncountably infinite. 
We've already seen some examples of this in class from the normal distribution!
We define the **probability density function**, or **PDF** of a continuous random variable `\(X \sim \mathcal{N}(\mu, \sigma^2)\)` as 
`$$f_X(x) = \dfrac{1}{\sqrt{2\pi\sigma^2}} e^{-\dfrac{1}{2}\left(\dfrac{x-\mu}{\sigma}\right)^2}$$`
Probability density functions must satisfy:

1. `\(f_X(x) &gt; 0\)` for all `\(x \in support(X)\)`
2. The area under the curve of the pdf in the support is equal to `\(1\)`. That is,
`\(\int_{support(X)} f_X(x)dx = 1\)`
3. If `\(A\)` is some interval in the support of `\(X\)`, then `\(P(X \in A) = \int_A f_X(x)dx\)`

Note that the second and third properties are essentially the continuous versions of the corresponding properties of discrete pmfs!

---

# Expected Value

The **expected value** or expectation of a random variable, denoted `\(E[X]\)` or `\(\mathbb{E}[X]\)`, is the mean of the random variable.
Intuitively, it can be thought of as a weighted average of all values in the support, weighted by their value in the pdf/pmf.

Expected values satisfy the following properties for random variables `\(X\)`, `\(Y\)` and constants `\(a\)`, `\(b\)`:
* `\(\mathbb{E}[a] = a\)` 
* `\(\mathbb{E}[aX + b] = a \mathbb{E}[X] + b\)`
* `\(\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y]\)`

If `\(X\)` and `\(Y\)` are independent, then 
* `\(\mathbb{E}[XY] = \mathbb{E}[X]\mathbb{E}[Y]\)`

---

# Variance

The **variance** of a random variable is the expected difference  between a random variable and its mean, squared.
`$$\mathbb{E}[(X - \mathbb{E}[X])^2]$$`
Intuitively, this measures how far the values of `\(X\)` are from their mean, on average.
It is a measure of spread, or variability.

Variances satisfy the following properties for a random variable `\(X\)` and constants `\(a, b\)`:
* `\(Var(a) = 0\)`
* `\(Var(aX + b) = a^2Var(X)\)`

The square root of the variance is known as the **standard deviation**, because it is the expected (standard) magnitude of the difference (deviation) between a random variable and its mean.
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "default",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
